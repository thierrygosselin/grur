% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/grur_imputations.R
\name{grur_imputations}
\alias{grur_imputations}
\title{Map-independent imputations of missing genotypes}
\usage{
grur_imputations(data, imputation.method = NULL,
  hierarchical.levels = "strata", pmm = 0, verbose = TRUE,
  parallel.core = parallel::detectCores() - 1,
  cpu.boost = parallel::detectCores()/2, random.seed = NULL,
  filename = NULL, ...)
}
\arguments{
\item{data}{A tidy genomic dataset.
It can be file in the working directory or
an object in the global environment.
To get a tidy dataset from various genomic format, see
\href{https://github.com/thierrygosselin/radiator}{radiator}
\code{\link[radiator]{tidy_genomic_data}}.
\emph{See details of this function for more info}.}

\item{imputation.method}{(character, optional)
Methods available for map-independent imputations of missing genotypes
(see details for more info):

\enumerate{
\item \code{imputation.method = "max"} Strawman imputation,
the most frequently observed genotypes (ties are broken at random).

\item \code{imputation.method = "rf"} On-the-fly-imputations using
Random Forests algorithm.

\item \code{imputation.method = "rf_pred"} Random Forests algorithm is used
as a prediction problem.

\item \code{imputation.method = "xgboost"} extreme gradient boosting trees
using depth-wise tree growth.

\item \code{imputation.method = "lightgbm"} for a light and fast 
leaf-wise tree growth gradient boosting algorithm (in devel).

\item \code{imputation.method = "bpca"} Multiple Correspondence Analysis (in devel).

\code{imputation.method = NULL} the function will stop.
Default: \code{imputation.method = NULL}.
}}

\item{hierarchical.levels}{(character, optional) \code{c("global", "strata")}.
Should the imputations be computed by markers globally or by strata.
Historically, this was \code{"populations"}.

Note that imputing genotype globally in conjunction with
\code{imputation.method = "max" or "strata"} can potentially create huge bias.
e.g. by introducing foreign genotypes/haplotypes in some strata/populations
(see note for more info).
Default: \code{hierarchical.levels = "strata"}.}

\item{pmm}{(integer, optional) Predictive mean matching used in conjunction with
random Forests and lightgbm (\code{imputation.method = "rf_pred"} or
\code{imputation.method = "lightgbm"}).
Number of candidate non-missing
value to sample from during the predictive mean matching step.
A fast k-nearest neighbor searching algorithms is used with this approach.
\code{pmm = 2} will use 2 neighbors.
Default: \code{pmm = 0}, will avoids this step.}

\item{verbose}{(optional, logical) When \code{verbose = TRUE}
the function is a little more chatty during execution.
Default: \code{verbose = TRUE}.}

\item{parallel.core}{(optional, integer) The number of core used for parallel.
For some algorithms, markers will be imputed in parallel and 
strata are processed sequentially.
Default: \code{parallel.core = parallel::detectCores() - 1}.}

\item{cpu.boost}{(optional, integer). Number of core for XGBoost and LightGBM.
For the best speed, set this to the number of real CPU cores,
not the number of threads. 
Most CPU using hyper-threading to generate 2 threads per CPU core.
Be aware that task manager or any CPU monitoring tool might report cores
not being fully utilized. This is normal.
Default: \code{cpu_boost = parallel::detectCores() / 2}.}

\item{random.seed}{(integer, optional) For reproducibility, set an integer
that will be used to initialize the random generator. With default,
a random number is generated. Currently not implemented for XGBoost and LightGBM.
Default: \code{random.seed = NULL}.}

\item{filename}{(optional) The function uses \code{\link[fst]{write.fst}},
to write the tidy data frame in
the working directory. The file extension appended to
the \code{filename} provided is \code{.rad}.
With default: \code{filename = NULL}, the imputed tidy data frame is
in the global environment only (i.e. not written in the working directory...).}

\item{...}{(optional) To pass further argument for fine-tuning your
imputations. See details below.}
}
\value{
The output in your global environment is the imputed tidy data frame.
If \code{filename} is provided, the imputed tidy data frame is also
written to the working directory.
}
\description{
The goal of this module is to provide a simple solution for
a complicated problem: missing genotypes in RADseq genomic datasets.
This function will performed \strong{map-independent imputations} of missing
genotypes.

\strong{Key features:}

\itemize{
\item \strong{Imputation methods: } several machine learning algorithms;
Random forests (predictive and on-the-fly-imputations); 
Extreme gradient tree boosting (XGBoost);
Fast, distributed, high performance gradient
boosting (LightGBM) and
Multiple Correspondence Analysis (MCA). Furthermore, the module allows to
compare these algorithms with the classic Strawman imputation
(~ max/mean/mode: the most frequently observed, i.e. non-missing, genotypes is used).
\item \strong{Hierarchical level: } Imputations conducted by strata or globally.
\item \strong{SNP linkage/Haplotypes: } Correlation among SNPs is accounted for during
rf and tree boosting imputations, i.e. the imputations are 
automatically conducted by haplotypes
when marker meta-information is available (chromosome, locus and position,
usually taken from VCF files). The alternative, considers all the markers 
independent and imputation is conducted by SNPs/markers.
\item \strong{Genotype likelihood (GL): } The GL info is detected automatically
(GL column in FORMAT field of VCF files). Genotypes with higher likelihood
will have higher probability during bootstrap samples of trees in Random
forests (under devel).
\item \strong{Predictive mean matching: } random forest in prediction mode
uses a fast k-nearest neighbor (KNN) searching algorithms 
(see argument documentation and details below).
\item \strong{Optimized for speed: } There's a
\href{https://github.com/thierrygosselin/grur#troubleshooting}{tutorial} and a
\href{https://github.com/thierrygosselin/grur#vignettes-and-examples}{vignette}
detailing the procedure to install the packages from source to enable OpenMP.
Depending on algorithm used, a progress bar is sometimes available to see 
if you have time for a coffee break!
}


Before running this function to populate the original dataset with synthetic
data, I highly recommend you look for patterns of missingness
\code{\link[grur]{missing_visualization}}
and explore the reasons for their presence 
(see \href{https://www.dropbox.com/s/4zf032g6yjatj0a/vignette_missing_data_analysis.nb.html?dl=0}{vignette}).
}
\details{
\strong{Predictive mean matching:}

Random Forests already behave like a nearest neighbor
classifier, with adaptive metric. Now we have the option to conduct
predictive mean matching on top of the prediction based missing value
imputation. PMM tries to raise the variance in the resulting conditional
distributions to a realistic level.
The closest k predicted values are identified by a fast
k-nearest neighbour approach wrapped in the package
\href{https://github.com/mayer79/missRanger}{missRanger}
Returned value correspond to the mean value.


\strong{haplotype/SNP approach:}

The \emph{haplotype approach} is automatically used when markers meta-information
is detected (chromosome/CHROM, locus/ID and SNP/POS columns, usually from a VCF file).
Missing genotypes from SNPs on the same locus or same RADseq read is undertaken
simulteneously to account for the correlation of the linked SNPs. When one or
more SNPs on the same read/haplotype is missing, the haplotype is deleted and
consequently, imputation might results in different genotype for those SNPs
that were not missing. This approach is much safer than potentially creating
weird chimeras during haplotype imputations.
Alternatively, a \emph{snp approach} is used, and the SNP are considered
independent. Imputations of genotypes is then conducted for each marker separately.


\strong{Imputing globally or by strata ?}
\code{hierarchical.levels = "global"} argument will act differently depending
on the \code{imputation.method} selected.

\strong{Strawman imputations (~ max/mean/mode) considerations: }

With \code{imputation.method = "max"} and \code{hierarchical.levels = "global"}
\emph{will likely create bias}.

\emph{Example 1 (unbalanced sample size):} Consider 2 populations evolving more
by drift than selection: pop1 (n = 36) and pop2 (n = 50).
You'll likely have a few polymorphic marker, where pop1 and pop2 are
monomorphic for different alleles (pop1 is fixed for the minor/ALT allele and
pop2 is fixed for the major/REF allele). Missing genotypes in pop1
using the most common filling technique in the literature (using mean/mode/max),
will result in pop1 having individuals with the REF allele.
Not something you want... unless your population membership is not 100% accurate,
(e.g. you might have migrants or wrong assignation),
which in this case you still don't want to impute with
\code{imputation.method = "max"} (see alternative below).

\emph{Example 2 (balanced sample size):} pop1 (n = 100) and pop2 (n = 100).
For a particular marker, pop1 as 85 individuals genotyped and pop2 100.
Again, if the populations are fixed for different alleles
(pop1 = ALT and pop2 = REF), you will end up having REF allele in your pop1,
not something you want... unless your population membership is not 100% accurate,
(e.g. you might have migrants or wrong assignation),
which in this case you still don't want to impute with
\code{imputation.method = "max"} (see alternative below).

\strong{Random Forests imputations: }

Random Forests use machine learning and you can take this into account while
choosing argument values. Uncertain of the groupings ? Use random forests with
\code{hierarchical.levels = "global"}. Random forests will account for the
potential linkage and correlation between
markers and genotypes to make the best imputation available. This can potentially
results in genotypes for a certain combo population/marker with new groupings
(e.g. a new allele). This is much more accurate and not the same thing as
the \code{imputation.method = "max"} because the imputed genotype was validated
after considering all the other genotype values of the individual being imputed.
\emph{Test the option and report bug if you find one.}

\emph{random forest with on-the-fly-imputation (rf): }the technique is described
in Tang and Ishwaran (2017). Non-missing genotypes are used for
the split-statistics. Daughter node assignation membership use random
non-missing genotypes from the inbag data. Missing genotypes are imputed at
terminal nodes using maximal class rule with out-of-bag non-missing genotypes.

\emph{random forest as a prediction problem (rf_pred): }markers with
missing genotypes are imputed one at a time. The fitted forest is used to
predict missing genotypes. Missingness in the response variables are
incorporated as attributes for growing the forest.

\emph{boosting trees:} Prediction method is used for both XGBoost and LightGBM.

\href{https://lightgbm.readthedocs.io/en/latest/index.html}{LightGBM documentation}

\href{http://xgboost.readthedocs.io/en/latest/}{XGBoost documentation}

\href{https://sites.google.com/view/lauraepp/parameters}{LightGBM and XGBoost parameters optimization}




\strong{... :dot dot dot arguments}

The argument is available to tailor your imputations using
XGBoost, LightGBM and Random Forests:

Available arguments for eXtreme Gradient Boosting tree method:
\emph{eta, gamma, max_depth, min_child_weight, subsample, colsample_bytree,
num_parallel_tree, nrounds, save_name, early_stopping_rounds}.
Refer to \code{\link[xgboost]{xgboost}} for arguments documentation.

Available arguments for LightGBM:
\emph{boosting, objective, learning_rate, feature_fraction, bagging_fraction,
bagging_freq, max_depth, min_data_in_leaf, num_leaves, early_stopping_rounds,
nrounds, max_depth, iteration.subsample}. Refer to \code{\link[lightgbm]{lightgbm}}
for arguments documentation. \code{iteration.subsample} is the number of iteration
to conduct training of the model with new subsamples (default: \code{iteration.subsample = 2}).

Available arguments for Random forests method:
\emph{num.tree, nodesize, nsplit, nimpute}.
Refer to \code{\link[randomForestSRC]{impute.rfsrc}} for arguments documentation.
}
\note{
\strong{Reference genome or linkage map available ?}
Numerous approaches are available and more appropriate, please search
the literature
(\href{https://online.papersapp.com/collections/05d6e65a-73c9-49e6-9c75-289a818f76f3/share}{references}).


\strong{What's the simple imputation message when running the function ?}

Before conducting the imputations by strata with the machine learning algorithms, 
the data is first screened for markers that are
monomorphic WITHIN the strata/populations.
Because for those cases, it's clear what the
missing genotypes should be, the imputations is very \emph{simple} and missing
genotypes are imputed with the only genotype found for the particular strata/populations.
There's no need for a fancy method here.
The small cost in time is worth it, because the model inside 
the machine learning algorithms will benefit from having a more complete and
reliable genotype matrix.


\strong{Deprecated arguments:}

\itemize{
\item \code{hierarchical.levels = "populations"} update your codes with "strata".
\item \code{imputations.group} is now replaced by \code{hierarchical.levels}
\item \code{impute} is no longer available.
Imputing using \code{impute = "allele"} option was wrong because it
was using F1 genotypes for imputations. Now imputation is only conducted at
the genotype level.
\item \code{iteration.rf} is no longer used. This argument is now available
inside the \code{...} for on-the-fly-imputations (see details). The default
is now set to 10 iterations.
\item \code{split.number} is automatically set.
}
}
\examples{
\dontrun{
# The simplest way to run when you have a tidy dataset:

wolf.imputed <- grur::grur_imputations(
data = "wolf.tidy.dataset.rad",
imputation.method = "lightgbm")

# This will impute the missing genotypes by strata (in this case the strata,
is the population), population using lightgbm.
# The remaining arguments will be the defaults.

# When you start with a vcf file you can use magrittr \%>\% to `pipe` the
# result. Below, an example with more arguments offered by the functions:

wolf.imp <- radiator::tidy_genomic_data(
    data = "batch_1.vcf",
    strata = "strata.wolf.10pop.tsv",
    vcf.metadata = TRUE,
    whitelist.markers = "whitelist.loci.txt",
    verbose = TRUE) \%>\%
grur::grur_imputations(
    data = ., imputation.method = "xgboost", parallel.core = 32)
}
}
\references{
Wright, M. N. & Ziegler, A. (2016).
ranger: A Fast Implementation of Random Forests for High Dimensional Data
in C++ and R.
Journal of Statistical Software, in press. http://arxiv.org/abs/1508.04409.

Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.

Chen T, Guestrin C. (2016).
XGBoost: A scalable tree boosting system. arXivorg. 2016.
doi:10.1145/2939672.2939785

Tang F, Ishwaran H. (2017) Random Forest Missing Data Algorithms.
arXiorg: 1â€“24.
}
\seealso{
The package \href{https://github.com/imbs-hl/ranger}{ranger}
(see Wright and Ziegler, 2016) provides a fast C++ version
of the original implementation of rf from Breiman (2001).

\href{https://github.com/mayer79/missRanger}{missRanger}

The package \href{https://github.com/dmlc/xgboost}{randomForestSRC} 
(see Tang and Ishwaran, 2017) provides
a fast on-the-fly imputation method.

\href{https://github.com/stekhoven/missForest}{missForest}

The package \href{https://github.com/dmlc/xgboost}{XGBoost} 
(Chen and Guestrin, 2016) provides
a fast C++ implementation for the extreme gradient tree boosting algorithm.

The package \href{https://github.com/Microsoft/LightGBM}{LightGBM} is an
exciting new algorithm to conduct tree boosting.
}
\author{
Thierry Gosselin \email{thierrygosselin@icloud.com}
}
